# === Marathi OCR Extraction Script ===
# Refactored with modular extractors + debug logging
# ===============================================
import fitz
from PIL import Image
import pytesseract
import pandas as pd
import numpy as np
from paddleocr import PaddleOCR
import re
import os, time, json
import cv2
import time
import pandas as pd
from sqlalchemy import create_engine,text,types
import logging
import pyodbc
logging.getLogger("ppocr").setLevel(logging.WARNING)

# --------------------------------------------
# =========== SQL Serve DB Config =============
# --------------------------------------------
DB_SERVER = "ORNET96"
DB_DRIVER = "ODBC Driver 17 for SQL Server"

DB_USER = "sa"                  # SQL Server username
DB_PASS = "manager"    # SQL Server password
DB_NAME = "KDMC"                # Default database (can be overridden)
TABLE_NAME = "Ward_Unknown" 

# Build connection string (ODBC)
connection_string = (
    f"mssql+pyodbc://{DB_USER}:{DB_PASS}@{DB_SERVER}/{DB_NAME}"
    f"?driver={DB_DRIVER.replace(' ', '+')}"
)

# Create SQLAlchemy engine with fast_executemany (better for bulk inserts)
engine = create_engine(connection_string, fast_executemany=True)

# -------------------------------------------
# =========== CONFIG(WORKSTATION2) ===========
# --------------------------------------------
# pdf_folder = r"D:\PYTHON DEVELOPMENT\Coorperation_OCR_Extraction\pdf_extract"
# temp_excel = r"D:\PYTHON DEVELOPMENT\Coorperation_OCR_Extraction\Output\output_temp1.xlsx"
# output_excel = r"D:\PYTHON DEVELOPMENT\Coorperation_OCR_Extraction\Output\tmc_details.xlsx"
# card_image_folder = r"D:\PYTHON DEVELOPMENT\Coorperation_OCR_Extraction\Card_Images"
# os.makedirs(card_image_folder, exist_ok=True)


# -------------------------------------------
# =========== CONFIG(ORNET91) ===========
# --------------------------------------------
pdf_folder = r"D:\Sahil_Tejam\ALL_OCR\Marathi_OCR\Input_Pdf"
temp_excel = r"D:\Sahil_Tejam\ALL_OCR\Marathi_OCR\Output_Sample\output_temp1.xlsx"
output_excel = r"D:\Sahil_Tejam\ALL_OCR\Marathi_OCR\Output_Sample\process_test1.xlsx"
card_image_folder = r"Extracted_Card_Img"
os.makedirs(card_image_folder, exist_ok=True)

# -------------------------------------------
# =========== PDF Render zoom ===========
# --------------------------------------------
zoom_factor = 3 


# -------------------------------------------
# =========== Tesseract OCR Setup ===========
# --------------------------------------------
pytesseract.pytesseract.tesseract_cmd = r"C:\Program Files\Tesseract-OCR\tesseract.exe"
tesseract_config = "--oem 1 --psm 11 -l mar"


# -------------------------------------------
# =========== Paddle OCR Setup ===========
# --------------------------------------------
ocr_paddle = PaddleOCR(use_angle_cls=True, lang='en', rec=True, gpu=True, precision='fp16', use_mp=True)


# ----------------------------------------
# ========= Prefix Mapping File =========
# ----------------------------------------
prefix_mapping_file = r"D:\Sahil_Tejam\ALL_OCR\HINDI_OCR\Prefix.xlsx"
sheet_name = "Dombivali-143"
valid_prefixes = set()

try:
    xls = pd.ExcelFile(prefix_mapping_file)
    if sheet_name in xls.sheet_names:
        prefix_df = pd.read_excel(prefix_mapping_file, sheet_name=sheet_name, dtype={"FirstThreeLetters": str})
        prefix_df["cnt"] = pd.to_numeric(prefix_df["cnt"], errors="coerce").fillna(0).astype(int)
        valid_prefixes = set(prefix_df["FirstThreeLetters"].astype(str))
except Exception as e:
    print(f"Error loading prefix mapping file: {e}")


# -------------------------------------------
# =========== DEBUG LOG Setup ===========
# --------------------------------------------
DEBUG = False
LOG_FILE = "debug.log"

with open(LOG_FILE, "w", encoding="utf-8") as f:
    f.write("=== OCR Debug Log ===\n")

def debug_log(msg):
    if DEBUG:
        print(msg)
        with open(LOG_FILE, "a", encoding="utf-8") as f:
            f.write(msg + "\n")

# -------------------------------------------------------------
# ========= Precompiled regex for Voter Name patterns =========
# -------------------------------------------------------------
VOTER_NAME_PATTERN = re.compile(
        r"(?:‡§Æ‡§§‡§¶‡§æ‡§∞‡§æ‡§ö‡•á|‡§â‡§≤‡§¶‡§æ‡§∞‡§æ‡§ö‡•á|‡§ü‡§®‡§¶‡§æ‡§∞‡§æ‡§ö‡•á|‡§ó‡•ç‡§Ø‡§æ|‡§Ü...|‡§Ü.|‡§Ü|‡§Æ‡§™ ‡§π‡•Å|‡§Æ‡§ü ‡§®‡•ç‡§®|‡§Æ‡§ü‡§®|‡§® ‡§π|‡§∞ ‡§õ‡•á|‡§Ö‡§∞ ‡§π‡•á)\s*(?:‡§™‡•Ç‡§∞‡•ç‡§£|‡§™‡•Å‡§∞‡•ç‡§£|‡§™‡•Ç‡§£|‡§â|‡§™‡•Ç‡§∞‡§£|‡§™‡§∞‡•ç‡§£)\s*[Ôºö:;Ôºõ]?\s*(.*?)(?=\s*(?:‡§®‡§æ‡§Ç‡§µ|‡§®‡§æ‡§µ|‡§µ‡§°‡§ø‡§≤‡§æ‡§Ç‡§ö‡•á\s*‡§®‡§æ‡§µ|‡§™‡§§‡•Ä‡§ö‡•á\s*‡§®‡§æ‡§µ|‡§Ü‡§à‡§ö‡•á\s*‡§®‡§æ‡§µ|‡§ò‡§∞\s*‡§ï‡•ç‡§∞‡§Æ‡§æ‡§Ç‡§ï|‡§µ‡§Ø|‡§≤‡§ø‡§Ç‡§ó|$))",
        re.IGNORECASE
    )

LEADING_JUNK_PATTERN = re.compile(r"^[^‡§Ö-‡§π‡•ò-‡•ü]+")
PUNCT_PATTERN        = re.compile(r"[‡•∞‡••]+")
MULTISPACE_PATTERN   = re.compile(r"\s+")
DIGIT_LATIN_PATTERN  = re.compile(r"^[0-9‡•¶-‡•Øa-zA-Z]+$")

# -------------------------------------------------------------
# ========= Precompiled regex for Relation Name patterns =========
# -------------------------------------------------------------
RELATION_KEYWORD_ALONE = re.compile(
    r"^(‡§™‡§§‡•Ä‡§ö‡•á ‡§®‡§æ‡§µ|‡§™‡§§‡§§‡•Ä‡§ö‡•á ‡§®‡§æ‡§µ|‡§™‡§§‡•ç‡§§‡•Ä‡§ö‡•á ‡§®‡§æ‡§µ|‡§µ‡§°‡§ø‡§≤‡§æ‡§Ç‡§ö‡•á ‡§®‡§æ‡§µ|‡§µ‡§°‡•Ä‡§≤|‡§Ü‡§à‡§ö‡•á ‡§®‡§æ‡§µ|‡§Ü‡§à‡§ö‡•á ‡§ö‡§æ‡§µ|‡§à‡§§‡§∞|‡§á‡§§‡§∞)$"
)

RELATION_PATTERNS = {
    "Husband": re.compile(r"(?:‡§™‡§§‡•Ä‡§ö‡•á ‡§®‡§æ‡§µ|‡§™‡§§‡§§‡•Ä‡§ö‡•á ‡§®‡§æ‡§µ|‡§™‡§§‡•ç‡§§‡•Ä‡§ö‡•á ‡§®‡§æ‡§µ)\s*[:Ôºö;]?\s*(.+)"),
    "Father":  re.compile(r"(?:‡§µ‡§°‡§ø‡§≤‡§æ‡§Ç‡§ö‡•á ‡§®‡§æ‡§µ|‡§µ‡§°‡•Ä‡§≤|‡§¨‡§ø‡§≤‡§æ‡§Ç‡§ö‡•á ‡§®‡§æ‡§§)\s*[:Ôºö;]?\s*(.+)"),
    "Mother":  re.compile(r"(?:‡§Ü‡§à‡§ö‡•á ‡§®‡§æ‡§µ)\s*[:Ôºö;]?\s*(.+)"),
    "Other":   re.compile(r"(?:‡§à‡§§‡§∞|‡§á‡§§‡§∞)\s*[:Ôºö;]?\s*(.+)")
}

# -------------------------------------------------------------
# ========= Precompiled regex for House Number patterns =========
# -------------------------------------------------------------
HOUSE_NUMBER_PATTERN = re.compile(r"(?:‡§ò‡§∞\s*‡§ï‡•ç‡§∞‡§Æ‡§æ‡§Ç‡§ï)[\s:'‚Äò‚Äô‚Äú‚Äù\-]*([0-9‡•¶-‡•Ø]+)", re.MULTILINE)
AGE_PATTERN          = re.compile(r"(?:‡§µ‡§Ø|‡§¨‡§Ø|‡§µ‡§Ø‡§Ç)[:;?\s]*([0-9‡•¶-‡•Ø<]+)")
NON_DIGIT_PATTERN    = re.compile(r"[^0-9‡•¶-‡•Ø]")  # remove junk in age


# -------------------------------------------------------------
# ========= Precompiled regex for Gender patterns =========
# -------------------------------------------------------------
AGE_PREFIX_PATTERN   = re.compile(r"‡§µ‡§Ø\s*[:;?\-]?\s*[^\s\n\r]*")
GENDER_PATTERN       = re.compile(r"(?:‡§≤‡§ø‡§Ç‡§ó|‡§≥‡§ø‡§Ç‡§ó|‡§≤‡§ø‡§ó|‡§õ‡§ø‡§Ç‡§ó|‡§†‡§ø‡§Ç‡§ó)\s*[:\-]?\s*([^\s\n\r:;]+)", re.IGNORECASE)
HOUSE_PREFIX_PATTERN = re.compile(r"(?:‡§ò‡§∞\s*‡§ï‡•ç‡§∞‡§Æ‡§æ‡§Ç‡§ï|‡§ò‡§∞\s*‡§ï‡•ç‡§∞\.?)\s*[^\s\n\r]*")

# Predefine known gender tokens
MALE_WORDS   = {"‡§™‡•Å"}
FEMALE_WORDS = {
    "‡§∏‡•ç‡§∞‡•Ä", "‡§∏‡•ç‡§§‡•ç‡§∞‡•Ä", "‡§∏‡§∞‡•Ä", "‡§ù‡§∞‡•Ä", "‡§ñ‡•ç‡§∞‡•Ä", "‡§ñ‡§∞‡•Ä",
    "‡§ñ‡•ç‡§§‡§∞‡•Ä", "‡§ñ‡•ç‡§§‡•ç‡§∞‡•Ä", "‡§ñ‡§§‡•ç‡§∞‡•Ä", "‡§ñ‡§§‡§∞‡•Ä", "‡§∏‡§§‡•ç‡§∞‡•Ä",
    "‡§ñ‡§∏‡§§‡•ç‡§∞‡•Ä", "‡§ñ‡§∏‡•ç‡§§‡§∞‡•Ä", "‡§ñ‡•ç‡§§‡•ç‡§∞‡§æ", "‡§∏‡•ç‡•ç‡§∞‡•Ä"
}
OTHER_WORDS  = {"‡§á‡§§‡§∞", "‡§à‡§§‡§∞", "‡§á‡•ç‡§§‡§∞"}


# -------------------------------------------------------------
# ========= Precompiled regex for Epic Number patterns =========
# -------------------------------------------------------------
EPIC_SLASH_PATTERN   = re.compile(r"^[A-Z]{2}/\d{2,}/\d{2,}/\d+$", re.IGNORECASE)
EPIC_3L7D_PATTERN    = re.compile(r"^[A-Z]{3}\d{7}$", re.IGNORECASE)
EPIC_2L8D_PATTERN    = re.compile(r"^[A-Z]{2}\d{8}$", re.IGNORECASE)
EPIC_PREFIX_DIGITS   = re.compile(r"^([A-Z]{2,4})(\d{7,8})$")
NON_ALNUM_SLASH      = re.compile(r"[^A-Za-z0-9/]")  # cleanup


# ------------------------------------------------------------------------------
# ========= Precompiled regex for assembly/list/serial numbers patterns =========
# ------------------------------------------------------------------------------
AC_NO_SEARCH     = re.compile(r"\b(\d{1,3})/\d{1,6}/\d{1,6}\b")
LIST_NO_SEARCH   = re.compile(r"\d{1,3}/(\d{1,5})/\d{1,5}\b")
SERIAL_NO_SEARCH = re.compile(r"\d{1,3}/\d{1,5}/(\d{1,5})\b")
LEADING_DIGITS   = re.compile(r"^(\d+)")
NON_DIGITS_SLASH     = re.compile(r"[^0-9/]")  # cleanup


# ================================================================
# ============ HELPERS =================
# ================================================================


# ------------------------------------------------------------
# ============ Marathi to English Number Conversion ==========
# ------------------------------------------------------------
def marathi_to_english_number(text):
    marathi_digits = "‡•¶‡•ß‡•®‡•©‡•™‡•´‡•¨‡•≠‡•Æ‡•Ø"
    english_digits = "0123456789"
    return text.translate(str.maketrans(marathi_digits, english_digits))


# ------------------------------------------------------------
# ============ Preprocessing for upscaling ==========
# ------------------------------------------------------------
def preprocess_image(img: Image.Image,upscale_factor=3) -> Image.Image:
    new_size = (img.width * upscale_factor, img.height * upscale_factor)
    upscaled_img = img.resize(new_size, Image.Resampling.LANCZOS)
    return upscaled_img


# ------------------------------------------------------------
# ============ Find Voter Card Boxes in Page Image ==========
# ------------------------------------------------------------
def find_card_boxes(pixmap_img, min_w=400, min_h=150, max_w=650, max_h=300, iou_thresh=0.3):
    """
    Detect voter card boxes from page image.
    - Filters duplicate inner/outer contours
    - Keeps only one bounding box per card
    """
    gray = cv2.cvtColor(pixmap_img, cv2.COLOR_RGB2GRAY)
    blur = cv2.GaussianBlur(gray, (5, 5), 0)
    edges = cv2.Canny(blur, 50, 150)

    contours, _ = cv2.findContours(edges, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)
    raw_boxes = []
    for cnt in contours:
        x, y, w, h = cv2.boundingRect(cnt)
        if min_w < w < max_w and min_h < h < max_h:
            raw_boxes.append((x, y, x + w, y + h))

    # Helper: IoU between two boxes
    def iou(boxA, boxB):
        xA = max(boxA[0], boxB[0])
        yA = max(boxA[1], boxB[1])
        xB = min(boxA[2], boxB[2])
        yB = min(boxA[3], boxB[3])
        interArea = max(0, xB - xA) * max(0, yB - yA)
        areaA = (boxA[2] - boxA[0]) * (boxA[3] - boxA[1])
        areaB = (boxB[2] - boxB[0]) * (boxB[3] - boxB[1])
        unionArea = float(areaA + areaB - interArea)
        return interArea / unionArea if unionArea > 0 else 0

    # Deduplicate: keep only one box per overlapping region
    deduped = []
    for b in sorted(raw_boxes, key=lambda b: (b[1], b[0])):  # scan row-wise
        if all(iou(b, d) < iou_thresh for d in deduped):
            deduped.append(b)

    # Sort again top-to-bottom, then left-to-right
    final_boxes = sorted(deduped, key=lambda b: (b[1] // 250, b[0]))

    return final_boxes


# -----------------------------------------------------------
# =========== Check if Voter Card is Present in Image ========
# ------------------------------------------------------------
def card_is_present(image, min_cards=1):
    """
    Returns True if at least `min_cards` voter cards are detected in the image.
    """
    boxes = find_card_boxes(np.array(image))  # PIL to NumPy for OpenCV
    return len(boxes) >= min_cards


# =================================================
# ============== HEADER EXTRACTORS ================
# =================================================


# --------------------------------------------------------------------------
# =============== Municipal Coorporation Extractors ==========================
# --------------------------------------------------------------------------
def extract_municipal(header_text):
    # match = re.search(r"([^\s]+)\s*‡§Æ‡§π‡§æ‡§®‡§ó‡§∞‡§™‡§æ‡§≤‡§ø‡§ï‡§æ", header_text)
    match = re.search(r"(.+?)\s*‡§Æ‡§π‡§æ‡§®‡§ó‡§∞‡§™‡§æ‡§≤‡§ø‡§ï‡§æ", header_text)
    municipal = match.group(1).strip() if match else ""
    # debug_log(f"[MUNICIPAL] {municipal}")
    return municipal


# ------------------------------------------------------------------------
# =============== Prabhag No and Name Extractors ==========================
# ------------------------------------------------------------------------
def extract_prabhag_info(text):
    prabhag_no, prabhag_name = "", ""
    prabhag_lines = []

    lines = text.splitlines()
    collecting = False

    max_additional_lines = 3  # safety max
    additional_lines_collected = 0
    blank_line_count = 0

    voter_data_pattern = re.compile(r"^\d{1,3}(,\d{1,4})?\s")

    # Match: ‡§™‡•ç‡§∞‡§≠‡§æ‡§ó ‡§ï‡•ç‡§∞ : <digits> - <name start>
    prabhag_pattern = re.compile(
        r"‡§™‡•ç‡§∞‡§≠‡§æ‡§ó\s*‡§ï‡•ç‡§∞\.?\s*[:\-]?\s*([‡•¶-‡•Ø0-9]+)\s*[-‚Äì‚Äî]\s*(.*)"
    )

    for idx, line in enumerate(lines):
        line_stripped = line.strip()

        if not collecting:
            match = prabhag_pattern.search(line_stripped)
            if match:
                prabhag_no = marathi_to_english_number(match.group(1))
                first_line = match.group(2).strip()
                if first_line:
                    prabhag_lines.append(first_line)
                collecting = True
            continue

        if collecting:
            # Stop if we reach voter rows
            if voter_data_pattern.match(line_stripped):
                break

            # Stop if a new header like "‡§Ø‡§æ‡§¶‡•Ä" or "‡§≠‡§æ‡§ó" starts
            if line_stripped.startswith("‡§Ø‡§æ‡§¶‡•Ä") or line_stripped.startswith("‡§≠‡§æ‡§ó"):
                break

            # Stop at blank lines
            if line_stripped == "":
                blank_line_count += 1
            else:
                blank_line_count = 0
                prabhag_lines.append(line_stripped)

            if blank_line_count >= 2:
                break

            additional_lines_collected += 1
            if additional_lines_collected >= max_additional_lines:
                break

    prabhag_name = " ".join(prabhag_lines).strip()
    return prabhag_no, prabhag_name


# ------------------------------------------------------------------------
# =============== Section No and Name Extractors ==========================
# ------------------------------------------------------------------------
def extract_section_info(text):
    section_no = ""
    section_lines = []

    lines = text.splitlines()
    collecting = False

    max_additional_lines = 10  # safety max lines after header
    additional_lines_collected = 0
    blank_line_count = 0

    voter_data_pattern = re.compile(r"^\d{1,3}(,\d{1,4})?\s")
    digit_pattern = re.compile(r"[0-9‡•¶-‡•Ø]")  # ‚úÖ matches English or Marathi digits

    def normalize_section_name(name: str) -> str:
        """Normalize OCR variants of 'NA' into 'NA'."""
        if not name or not name.strip():
            return "NA"

        cleaned = name.strip().lower().replace(" ", "").replace(".", "")

        # ‚úÖ Add all known weird variants
        na_variants = {
            "na", "n/a", "‡§è‡§®‡§è", "nil", "none", "---",
            "1‡•Ø%", "1‡•´/", "1‡•Ø/", "1‡•Ø¬ª", "1‡•Ø%","1‡•Ø", "10¬ª", "1¬ª", "1‡•Ø‡•¶"
        }

        if cleaned in na_variants:
            return "NA"

        return name.strip()

    for idx, line in enumerate(lines):
        line_stripped = line.strip()

        if not collecting:
            match = re.search(
                r"‡§Ø‡§æ‡§¶‡•Ä\s*‡§≠‡§æ‡§ó\s*‡§ï‡•ç‡§∞\.?\s*[‡•¶-‡•Ø0-9]+\s*[:\-]\s*([‡•¶-‡•Ø0-9]+)\s*-\s*(.*)",
                line_stripped
            )
            if match:
                section_no = marathi_to_english_number(match.group(1))
                first_line = match.group(2).strip()
                if first_line and not digit_pattern.search(first_line):  
                    section_lines.append(first_line)
                collecting = True
            continue

        if collecting:
            if voter_data_pattern.match(line_stripped):
                break
            if digit_pattern.search(line_stripped):
                break

            if line_stripped == "":
                blank_line_count += 1
            else:
                blank_line_count = 0
                section_lines.append(line_stripped)

            if '.' in line_stripped:
                break
            if blank_line_count >= 2:
                break

            additional_lines_collected += 1
            if additional_lines_collected >= max_additional_lines:
                break

    # ‚úÖ If nothing captured, return NA
    if not section_lines:
        return section_no, "NA"

    section_name = " ".join(section_lines).strip()
    section_name = normalize_section_name(section_name)
    return section_no, section_name


# ------------------------------------------------------------------------
# =============== Booth Name and Address Extractors ==========================
# ------------------------------------------------------------------------
# def extract_booth_name(header_text):
#     booth_name = ""
#     booth_match = re.search(r"‡§Æ‡§§‡§¶‡§æ‡§®\s*‡§ï‡•á‡§Ç‡§¶‡•ç‡§∞\s*[:\-]\s*([^\n]+)", header_text)
#     if booth_match:
#         booth_text = booth_match.group(1).strip()
#         if "‡§™‡§§‡•ç‡§§‡§æ" in booth_text:
#             booth_name = booth_text.split("‡§™‡§§‡•ç‡§§‡§æ")[0].strip()
#         else:
#             booth_name = booth_text
#     booth_name = re.sub(r"[A-Za-z0-9@#%^&*_=+\[\]{}<>;:.,/\\|-]", "", booth_name).strip()
#     debug_log(f"[BOOTH_NAME] {booth_name}")
#     return booth_name

# def extract_booth_address(header_text):
#     booth_address = ""
#     addr_match = re.search(r"‡§™‡§§‡•ç‡§§‡§æ\s*[:\-]\s*([^\n]+)", header_text)
#     if addr_match:
#         booth_address = addr_match.group(1).strip()
#     booth_address = re.sub(r"[A-Za-z0-9@#%^&*_=+\[\]{}<>;:.,/\\|-]", "", booth_address).strip()
#     debug_log(f"[BOOTH_ADDR] {booth_address}")
#     return booth_address


# --------------------------------------------------------------
# =============== Extract Header Info from Page Image ============
# --------------------------------------------------------------
def extract_header_info(page_img, top_margin, zoom_factor):
    header_crop = page_img.crop((0, 0, page_img.width, int(top_margin * zoom_factor)))
    header_text = pytesseract.image_to_string(header_crop, config="--psm 6 -l mar").strip()
    # debug_log(f"[HEADER RAW]\n{header_text}")
    municipal = extract_municipal(header_text)
    section_no, section_name = extract_section_info(header_text)
    # booth_name = extract_booth_name(header_text)
    # booth_address = extract_booth_address(header_text)
    prabhag_no, prabhag_name = extract_prabhag_info(header_text)
    return {
        "Municipal_Corporation": municipal,
        "Prabhag_No": prabhag_no,
        "Prabhag_Name": prabhag_name,
        # "Section_No": section_no,
        "Section_No": section_no,
        "Section_Name": section_name,
        # "Booth_Name": booth_name,
        # "Booth_Address": booth_address,
        "Raw_Header_Text": header_text
    }

def extract_pdf_header_info(pdf_file, zoom_factor):
    with fitz.open(pdf_file) as doc:
        first_page = doc[0]
        pix = first_page.get_pixmap(matrix=fitz.Matrix(zoom_factor, zoom_factor))
        img = Image.frombytes("RGB", [pix.width, pix.height], pix.samples)

    hdr = extract_header_info(img, top_margin=118.0, zoom_factor=zoom_factor)
    pdf_header_info = {
        "Municipal_Corporation": hdr.get("Municipal_Corporation", ""),
        "Prabhag_No": hdr.get("Prabhag_No", ""),
        "Prabhag_Name": hdr.get("Prabhag_Name", ""),
        "File_Name": os.path.basename(pdf_file)
    }
    return pdf_header_info


# ================================================================
# ================== VOTER CARD EXTRACTORS =======================
# ===============================================================


# -----------------------------------------------------
# =========== Clean Marathi Name Extraction ============
# -----------------------------------------------------
def clean_name(name, max_words=None):
    """
    Clean Marathi or OCR-extracted name:
    - Remove quotes, dashes, colons, digits, slashes, dots
    - Remove stray single characters like '‡§±‡•ç‡§Ø‡§æ', '‡§≤', etc.
    - Limit to max_words if specified
    """
    if not name:
        return ""
    
    # Remove leading/trailing quotes, dashes, colons, slashes, dots, digits
    name = re.sub(r"^[\"‚Äú‚Äù‚Äò‚Äô(\-*:0-9\/,\.&‡••]+", "", name)
    # name = re.sub(r"[\"‚Äú‚Äù‚Äò‚Äô\-*:0-9\/,\.]+$", "", name)
    name = re.sub(r"^[^‡§Ö-‡§π]+", "", name)   # leading junk
    # name = re.sub(r"[^‡§Ö-‡§π\s]+$", "", name) # trailing junk
    name = re.sub(r"^[^‡§Ö-‡§π‡•ò-‡•ü]+", "", name)  # leading junk (extended)

    
    # Remove stray single-character artifacts
    words = name.split()
    words = [w for w in words if len(w) > 1]  # keep words with length > 1

    # Limit to max_words
    words = words[:max_words]
    
    return " ".join(words).strip()


# -----------------------------------------------------------------
# =========== Extract Index Number from PaddleOCR Text ============
# ------------------------------------------------------------------
def extract_index_number(paddle_text):
    """
    Extract index number from PaddleOCR text.
    - Supports formats like '143/19/861', '7,281', '208.75.995'
    - Skips alphanumeric lines (EPIC numbers like HTQ1428796)
    - Returns string (digits + / allowed)
    """
    index_number = ""
    
    lines = [line.strip() for line in paddle_text.splitlines() if line.strip()]
    
    for line in lines:
        # Skip if line has any English letters (likely EPIC or junk)
        if re.search(r"[A-Za-z]", line):
            continue

        # Keep digits, / , and . , but remove all other junk
        cleaned = re.sub(r"[^\d/.,]", "", line)

        # Must contain at least one digit
        if not re.search(r"\d", cleaned):
            continue

        # Normalize: remove commas and dots, but keep slashes
        normalized = cleaned.replace(",", "").replace(".", "")

        if re.match(r"^[\d,\.]+$", cleaned):
            normalized = cleaned.replace(",", "").replace(".", "")
            if normalized.isdigit():
                index_number = normalized
                # debug_log(f"[INDEX_NUM] Extracted (plain): {index_number} from {line}")
                print(f"[INDEX_NUM] Extracted (plain): {index_number} from {line}")
                
                return index_number        

        # # Accept only if it looks like a valid index number (digits with optional / separators)
        # if re.match(r"^\d+(?:/\d+)*$", normalized):
        #     index_number = normalized
        #     debug_log(f"[INDEX_NUM] Extracted: {index_number} from line: {line}")
        #     break
        
    return index_number


# -----------------------------------------------------
# =========== Correct Name using Dictionary ============
# -----------------------------------------------------
with open("corrections.json", "r", encoding="utf-8") as f:
    correction_dict = json.load(f)

def correct_name_with_dict(name: str) -> str:
    """Correct common OCR mistakes using dictionary from JSON."""
    return " ".join([correction_dict.get(word, word) for word in name.split()])


# -----------------------------------------------------
# =========== Clean Tesseract OCR Noise ============
# -----------------------------------------------------
def clean_tesseract_text(text: str) -> str:
    """
    Remove fixed OCR noise patterns like '‡§Æ‡§ø‡•™123456' or '‡§Æ‡§ø‡§è123456' from Tesseract text.
    """

    import re
    # Remove '‡§Æ‡§ø‡•™' + digits
    text = re.sub(r"\b‡§Æ‡§ø‡•™\d+\b", "", text)

    # Remove '‡§Æ‡§ø‡§è' + digits
    text = re.sub(r"‡§Æ‡§ø‡§è8‡••81016", "", text)
    
    text = re.sub(r"‡§Æ‡§ø‡§è818016","", text)
    # Cleanup extra spaces / blank lines
    text = re.sub(r"[ ]{2,}", " ", text)
    text = re.sub(r"\n\s*\n", "\n", text)

    return text.strip()


# ------------------------------------------------------------------
# =========== Extract Voter Name from Tesseract OCR Text ============
# ------------------------------------------------------------------
def extract_voter_name(text):
    """Extract full voter name from Marathi OCR text (optimized)."""
    # Clean basic unwanted chars
    text = text.replace("\n", " ").replace("[", "").replace("]", "").replace("'", "")
    text = MULTISPACE_PATTERN.sub(" ", text).strip()

    # Match name
    match = VOTER_NAME_PATTERN.search(text)
    if not match:
        return ""

    main_text = match.group(1).strip()
    main_text = LEADING_JUNK_PATTERN.sub("", main_text)     # remove junk at start
    main_text = PUNCT_PATTERN.sub("", main_text).strip()    # remove Marathi punctuation
    main_text = MULTISPACE_PATTERN.sub(" ", main_text)      # collapse spaces

    # Drop numbers/Latin-only tokens
    words = [w for w in main_text.split() if not DIGIT_LATIN_PATTERN.match(w)]
    voter_name = " ".join(words[:4])  # keep first 3‚Äì4 words

    # Apply dictionary correction
    return correct_name_with_dict(voter_name)


# ---------------------------------------------------------------
# =========== Split Marathi Full Name into First/Last ===========
# ----------------------------------------------------------------
def split_relation_name(full_name):
    words = full_name.strip().split()
    first = words[1] if len(words) >= 2 else ""
    last = words[0] if words else ""
    # debug_log(f"[NAME_SPLIT] First={first}, Last={last}")
    return first, last

def split_voter_name(full_name: str):
    """
    Split Marathi full name into First, Last, Middle.
    Convention: <Last> <First> <Middle/Father's Name>
    Example: '‡§™‡§æ‡§ü‡•Ä‡§≤ ‡§∏‡•Å‡§∞‡•á‡§∂ ‡§Æ‡§π‡§æ‡§¶‡•á‡§µ' -> First='‡§∏‡•Å‡§∞‡•á‡§∂', Last='‡§™‡§æ‡§ü‡•Ä‡§≤', Middle='‡§Æ‡§π‡§æ‡§¶‡•á‡§µ'
    """
    words = full_name.strip().split()
    
    if not words:
        return "", "", ""
    
    if len(words) == 1:
        # Only one word: assume it's the first name
        return words[0], "", ""
    
    if len(words) == 2:
        # Two words: assume <Last> <First>
        last, first = words
        return first, last, ""
    
    # Three or more words: assume <Last> <First> <Middle...>
    last, first, *middle = words
    middle = " ".join(middle)  # Join remaining words in case of 4+
    
    return first, last, middle

# --------------------------------------------------------
# =========== Extract Relation Type and Name ============
# --------------------------------------------------------
def extract_relation_info(text):
    """Extract relation type and full relation name from Marathi OCR text (optimized)."""

    # --- Normalize lines ---
    lines = [l.strip() for l in text.splitlines() if l.strip()]
    merged_lines = []
    i = 0
    while i < len(lines):
        line = lines[i]

        # Case: keyword alone (next line is value)
        if RELATION_KEYWORD_ALONE.match(line):
            if i + 1 < len(lines):
                next_val = lines[i + 1].strip()
                merged_lines.append(f"{line} {next_val}")
                i += 2
                continue

        merged_lines.append(line)
        i += 1

    clean_text = "\n".join(merged_lines)

    # --- Regex patterns for relation types ---
    for r_type, pattern in RELATION_PATTERNS.items():
        match = pattern.search(clean_text)
        if match:
            relation_name = match.group(1).lstrip()   # remove leading spaces
            relation_name = clean_name(relation_name, max_words=2)
            relation_name = correct_name_with_dict(relation_name)
            return r_type, relation_name

    return "", ""


# --------------------------------------------------------
# =========== Limit Relation Name to 2 Words ============
# --------------------------------------------------------
def limit_to_two_words(name):
    """Limit extracted relation name to max 2 words."""
    return " ".join(name.split()[:2]) if name else ""


# --------------------------------------------------------------------
# =========== Extract House Number from Tesseract OCR Text ============
# --------------------------------------------------------------------
def extract_house_number(text):
    """Extract house number (Devanagari or English digits)."""
    match = HOUSE_NUMBER_PATTERN.search(text)
    return marathi_to_english_number(match.group(1).strip()) if match else "NA"


# ------------------------------------------------------------
# =========== Extract Age from Tesseract OCR Text ============
# ------------------------------------------------------------
def extract_age(text):
    """Extract age in Marathi and English digits."""
    match = AGE_PATTERN.search(text)
    if not match:
        # print("[AGE] Not found")
        return "", ""

    age_marathi_raw = match.group(1).strip()
    age_marathi_clean = NON_DIGIT_PATTERN.sub("", age_marathi_raw)  # remove < or other junk
    age_english = marathi_to_english_number(age_marathi_clean)

    # print(f"[AGE] Marathi={age_marathi_clean} English={age_english}")
    return age_marathi_clean, age_english


# ---------------------------------------------------------------
# =========== Extract Gender from Tesseract OCR Text ============
# ---------------------------------------------------------------
def extract_gender(text):
    """Extract gender (raw + normalized) from OCR text (optimized)."""

    # Step 1: Find text after '‡§µ‡§Ø'
    age_match = AGE_PREFIX_PATTERN.search(text)
    if age_match:
        after_age_text = text[age_match.end():]
        gender_match = GENDER_PATTERN.search(after_age_text)

        raw_gender = gender_match.group(1).strip() if gender_match else ""
        normalized_gender = classify_gender(raw_gender)

        if normalized_gender:  # Found via ‡§µ‡§Ø‚Üí‡§≤‡§ø‡§Ç‡§ó
            # print(f"[RAW] {raw_gender} ‚Üí [NORMALIZED] {normalized_gender}")
            return raw_gender, normalized_gender

    # === Fallback: look after '‡§ò‡§∞ ‡§ï‡•ç‡§∞‡§Æ‡§æ‡§Ç‡§ï' ===
    house_match = HOUSE_PREFIX_PATTERN.search(text)
    if house_match:
        after_house_text = text[house_match.end():]

        for word in MALE_WORDS | FEMALE_WORDS | OTHER_WORDS:
            if word in after_house_text:
                raw_gender = word
                normalized_gender = classify_gender(raw_gender)
                # print(f"[FALLBACK RAW] {raw_gender} ‚Üí [NORMALIZED] {normalized_gender}")
                return raw_gender, normalized_gender

    return "", ""


def classify_gender(raw_text: str) -> str:
    """Normalize OCR variants into standard Marathi root words."""
    raw_text = raw_text.strip().lower()

    if any(word in raw_text for word in MALE_WORDS):
        return "‡§™‡•Å‡§∞‡•Å‡§∑"
    if any(word in raw_text for word in FEMALE_WORDS):
        return "‡§∏‡•ç‡§§‡•ç‡§∞‡•Ä"
    if any(word in raw_text for word in OTHER_WORDS):
        return "‡§á‡§§‡§∞"

    return ""


def marathi_to_english_gender(normalized_gender: str) -> str:
    return {"‡§™‡•Å‡§∞‡•Å‡§∑": "Male", "‡§∏‡•ç‡§§‡•ç‡§∞‡•Ä": "Female", "‡§á‡§§‡§∞": "Other"}.get(normalized_gender, "")


# ---------------------------------------------------------------------
# =========== Parse Voter Card Info from Tesseract OCR Text ============
# ---------------------------------------------------------------------
def parse_voter_card(marathi_text, cleaned_text):
    voter_name = extract_voter_name(marathi_text)
    voter_first, voter_last, voter_middle = split_voter_name(voter_name)
    relation_type, relation_name = extract_relation_info(cleaned_text)
    rel_first, rel_last = split_relation_name(relation_name)
    house_number = extract_house_number(cleaned_text)
    age_marathi, age_english = extract_age(cleaned_text)
    raw_gender, normalized_gender = extract_gender(cleaned_text)
    gender_english = marathi_to_english_gender(normalized_gender)

    return {
        "Voter_Name": voter_name,
        "Voter_First_Name": voter_first,
        "Voter_Middle_Name": voter_middle,
        "Voter_Last_Name": voter_last,
        "Relation_Type": relation_type,
        "Relation_Name": relation_name,
        "Relation_First_Name": rel_first,
        "Relation_Last_Name": rel_last,
        "House_Number": house_number,
        "Age_Marathi": age_marathi,
        "Age_English": age_english,
        "Gender_Marathi": normalized_gender,  # normalized Marathi root word
        "Gender_English": gender_english,     # English category
    }


# ================================================================
# === EPIC & SERIAL EXTRACTORS ===
# ================================================================

# ---------------------------------------------------------------------------
# =========== Extract and Correct EPIC Number from PaddleOCR Text ============
# ---------------------------------------------------------------------------
def extract_epic_number(paddle_text: str) -> str:
    """
    Extract EPIC number from PaddleOCR text.
    Supports:
      - Structured format: MT/03/017/0050016
      - Old format: RSC9492026 (3 letters + 7 digits)
      - New format: HT01523646 (2 letters + 8 digits)
    """
    epic_number = ""
    lines = [NON_ALNUM_SLASH.sub("", line).strip() for line in paddle_text.splitlines()]
    lines = [line for line in lines if line]

    for line in lines:
        if EPIC_SLASH_PATTERN.match(line):
            epic_number = line.upper()
            break
        elif EPIC_3L7D_PATTERN.match(line):
            epic_number = line.upper()
            break
        elif EPIC_2L8D_PATTERN.match(line):
            epic_number = line.upper()
            break

    return epic_number


def correct_epic_number(epic_number: str) -> str | None:
    """
    Normalize EPIC numbers:
      - Keep slash-format as-is
      - Always return 3-letter prefix + 7-digit number
      - If OCR gives 2 letters/8 digits, try prefix mapping
      - If prefix mapping fails, return None
    """
    if not epic_number:
        return None

    epic_number = epic_number.strip().upper()

    # Case 1: Slash type ‚Üí keep as-is
    if "/" in epic_number:
        return epic_number

    # Case 2: Match prefix + digits
    m = EPIC_PREFIX_DIGITS.match(epic_number)
    if not m:
        return None  # Invalid format

    prefix, digits = m.group(1), m.group(2)
    digits = digits[-7:]  # always last 7 digits

    if prefix in valid_prefixes and len(prefix) == 3:
        return prefix + digits

    if len(prefix) == 2:
        best = find_best_prefix(prefix)
        return (best + digits) if best else None

    if len(prefix) >= 3:
        best = find_best_prefix(prefix[:2])
        return (best + digits) if best else prefix[:3] + digits

    return None


# --------------------------------------------------------
# =========== Find Closest Valid Prefix for EPIC ============
# --------------------------------------------------------
def find_best_prefix(prefix: str) -> str | None:
    """Map a 2-letter prefix to a valid 3-letter prefix."""
    prefix_2 = prefix[:2]
    matches = [p for p in valid_prefixes if p.startswith(prefix_2)]
    return sorted(matches, key=lambda x: (len(x), x), reverse=True)[0] if matches else None


# --------------------------------------------------------
# =========== Extract Assembly/List/Serial Numbers ============
# --------------------------------------------------------
def extract_assembly_consitution_no(paddle_text: str) -> str:
    """
    Extract Assembly Constituency No (first part of 188/36/12 -> 188).
    Uses a single regex scan over the text.
    """
    match = AC_NO_SEARCH.search(paddle_text)
    return match.group(1) if match else ""


def extract_list_number(paddle_text: str) -> str:
    """
    Extract List number (middle part of 188/36/12 -> 36).
    """
    match = LIST_NO_SEARCH.search(paddle_text)
    list_number = match.group(1) if match else ""
    if list_number:
        debug_log(f"[LIST] Extracted={list_number}")
    return list_number


def extract_serial_number(paddle_text: str, extracted_text: str, serial_counter: int, ocr_empty: bool):
    """
    Extract Serial number:
    - First tries from fraction style like 188/36/12 (‚Üí last part)
    - If not found, fallback to sequential numbering
    """
    match = SERIAL_NO_SEARCH.search(paddle_text)
    if match:
        serial_number = match.group(1)
        serial_source_text = f"Fraction style: {match.group(0)}"
        return serial_number, serial_source_text, serial_counter

    # Fallback ‚Üí sequential numbering
    if not ocr_empty and extracted_text.strip():
        serial_match = LEADING_DIGITS.match(extracted_text.strip())
        serial_number = int(serial_match.group(1)) if serial_match else serial_counter
        serial_source_text = serial_match.group(0) if serial_match else "Not Found - Assigned Sequentially"
        serial_counter += 1
    elif not ocr_empty:
        serial_number, serial_source_text = serial_counter, "Not Found - Assigned Sequentially"
        serial_counter += 1
    else:
        serial_number, serial_source_text = None, "Skipped - Empty OCR"

    return serial_number, serial_source_text, serial_counter


# ---------------------------------------------------------------
# ================ File Name Extractor ================
# ---------------------------------------------------------------
def get_file_name(pdf_file):
    """
    Extract only the file name (without extension) from a PDF path.
    Example: C:/folder/Booth9.pdf -> Booth9
    """
    return os.path.splitext(os.path.basename(pdf_file))[0]


# ---------------------------------------------------------------
# ============= Emergency Save Helpers ===============
# ---------------------------------------------------------------
def save_progress(voter_details, column_order, temp_excel):
    """Save partial progress to a temp Excel file."""
    if not voter_details:
        return
    df = pd.DataFrame(voter_details)
    df = df[[col for col in column_order if col in df.columns]]
    df.to_excel(temp_excel, index=False, engine="openpyxl")  # ‚úÖ removed encoding
    print(f"üíæ Progress saved in temp file: {temp_excel}")


def finalize_output(temp_excel, output_excel):
    """Finalize output by renaming temp file ‚Üí final file and removing temp."""
    if os.path.exists(temp_excel):
        os.replace(temp_excel, output_excel)  # atomic replace
        print(f"‚úÖ Final file saved at: {output_excel}")
        try:
            os.remove(temp_excel)  # cleanup (on Windows os.replace already moves it)
            print("üóëÔ∏è Temp file deleted")
        except FileNotFoundError:
            pass
    else:
        print("‚ö†Ô∏è Temp file not found. Nothing to finalize.")


column_order = [
    "File_Name","New_Voter_ID","Municipal_Corporation", "Prabhag_No", "Prabhag_Name",
    "Voter_ID", "Section_No", "Section_Name","List_Number","Page",
    "Ac_no","EPIC_Number",
    "Voter_Name", "Voter_First_Name", "Voter_Middle_Name", "Voter_Last_Name",
    "Relation_Type", "Relation_Name", "Relation_First_Name", "Relation_Last_Name",
    "House_Number",
    "Age_Marathi", "Age_English",
    "Gender_Marathi", "Gender_English",
    # "Booth_Name", "Booth_Address",
     "Card_Index",
    "Marathi_Text","Cleaned_Text", "Paddle_Text",
]


# ---------------------------------------------------------------
# ================ Main Page Processing Function ===============
# ---------------------------------------------------------------
def process_page(pdf_file, page_num, zoom_factor, pdf_header_info):
    """
    Process a single page and return parsed voter rows.
    Now also attaches Municipal/Prabhag/File_Name per row directly.
    """
    voter_details = []
    serial_counter = 1

    # Open page
    doc = fitz.open(pdf_file)
    page = doc[page_num - 1]
    pix = page.get_pixmap(matrix=fitz.Matrix(zoom_factor, zoom_factor))
    full_img = Image.frombytes("RGB", [pix.width, pix.height], pix.samples)

    # === Extract header info (includes Section_No / Section_Name) ===
    header_info = extract_header_info(full_img, top_margin=118.0, zoom_factor=zoom_factor)

    section_no = header_info["Section_No"]
    section_name = header_info["Section_Name"]
    raw_header = header_info["Raw_Header_Text"]

    print(f"üìå Page {page_num} Header ‚Üí Section_No: {section_no} | Section_Name: {section_name}")

    # Convert to numpy
    pix_np = np.frombuffer(pix.samples, dtype=np.uint8).reshape(pix.height, pix.width, pix.n)
    if pix_np.shape[2] == 4:
        pix_np = pix_np[:, :, :3]

    # Detect voter card boxes
    card_coords_points = find_card_boxes(pix_np)
    if not card_coords_points:
        print(f"‚ö†Ô∏è No card boxes detected on page {page_num}")
        doc.close()
        return []

    # OCR each voter card
    for card_index, (x1, y1, x2, y2) in enumerate(card_coords_points, start=1):
        card_img = full_img.crop((x1, y1, x2, y2))

        preprocessed_img = preprocess_image(card_img)
        marathi_text = pytesseract.image_to_string(preprocessed_img, config=tesseract_config).strip()
        if not marathi_text.strip():
            continue

        cleaned_text = clean_tesseract_text(marathi_text)

        result_paddle = ocr_paddle.ocr(np.array(preprocessed_img))
        paddle_text = "\n".join([line[1][0] for line in result_paddle[0]]) if result_paddle and result_paddle[0] else ""

        voter_name = extract_voter_name(cleaned_text)
        parsed = parse_voter_card(marathi_text, cleaned_text)

        epic_number = extract_epic_number(paddle_text)
        list_number = extract_list_number(paddle_text)
        ac_no = extract_assembly_consitution_no(paddle_text)
        index_number = extract_index_number(paddle_text)

        serial_number, _, serial_counter = extract_serial_number(
            paddle_text, paddle_text, serial_counter, False
        )

        parsed.update({
            "New_Voter_ID": index_number,
            "EPIC_Number": correct_epic_number(epic_number) if epic_number else None,
            "Voter_ID": serial_number,
            "Page": page_num,
            "Card_Index": card_index,
            "Marathi_Text": marathi_text,
            "Cleaned_Text": cleaned_text,
            "Voter_Name": voter_name,
            "Paddle_Text": paddle_text,
            "List_Number": list_number,
            "Ac_no": ac_no,
            # ‚úÖ Always attach section info
            "Section_No": section_no,
            "Section_Name": section_name,
            "Raw_Header_Text": raw_header,
            # ‚úÖ Also attach PDF-level header info here (new change)
            "Municipal_Corporation": pdf_header_info.get("Municipal_Corporation", ""),
            "Prabhag_No": pdf_header_info.get("Prabhag_No", ""),
            "Prabhag_Name": pdf_header_info.get("Prabhag_Name", ""),
            "File_Name": pdf_header_info.get("File_Name", os.path.basename(pdf_file)),
        })

        voter_details.append(parsed)

    doc.close()
    return voter_details

# ---------------------------------------------------------------
# ================ Checkpointing Helpers ===============    
# ---------------------------------------------------------------
CHECKPOINT_FILE = "checkpoint.json"

def load_checkpoint():
    if os.path.exists(CHECKPOINT_FILE):
        with open(CHECKPOINT_FILE, "r", encoding="utf-8") as f:
            return json.load(f)
    return {}

def save_checkpoint(pdf_name, page_num, temp_excel):
    checkpoint = load_checkpoint()
    checkpoint[pdf_name] = {
        "last_page": int(page_num),  # store page as int for JSON
        "temp_excel": temp_excel
    }
    with open(CHECKPOINT_FILE, "w", encoding="utf-8") as f:
        json.dump(checkpoint, f, indent=2)
    print(f"üíæ Checkpoint saved for {pdf_name} at page {page_num}")


# ---------------------------------------------------------------
# ================ SQL Server Insertion Helpers ===============
# ---------------------------------------------------------------    

# === Create SQLAlchemy Engine ===
def get_engine(db_name, user=DB_USER, password=DB_PASS):
    """
    Create SQLAlchemy engine using SQL Server Authentication.
    """
    conn_str = f"mssql+pyodbc://{user}:{password}@{DB_SERVER}/{db_name}?driver=ODBC+Driver+17+for+SQL+Server"
    return create_engine(conn_str, fast_executemany=True)

# === Ensure Database Exists ===
def ensure_database_exists(db_name, user=DB_USER, password=DB_PASS):
    """
    Creates the database if it doesn't exist using raw pyodbc (autocommit=True).
    Avoids 'CREATE DATABASE inside transaction' error.
    """
    conn_str = f"DRIVER={{ODBC Driver 17 for SQL Server}};SERVER={DB_SERVER};UID={user};PWD={password};DATABASE=master"
    with pyodbc.connect(conn_str, autocommit=True) as conn:
        cursor = conn.cursor()
        cursor.execute(f"""
            IF NOT EXISTS (SELECT name FROM sys.databases WHERE name='{db_name}')
                CREATE DATABASE [{db_name}]
        """)
        print(f"‚úÖ Database ready: {db_name}")

# === Clean + Validate Integer Columns ===
def enforce_integer_columns(df, int_cols):
    for col in int_cols:
        if col in df.columns:
            numeric_series = pd.to_numeric(df[col], errors="coerce")
            bad_mask = numeric_series.isna() & df[col].notna()
            if bad_mask.any():
                bad_vals = df[col][bad_mask].unique()
                raise ValueError(f"‚ùå Column '{col}' contains non-integer values: {bad_vals}")
            if not (numeric_series.dropna() == numeric_series.dropna().astype(int)).all():
                bad_vals = df[col][numeric_series != numeric_series.astype(int)].unique()
                raise ValueError(f"‚ùå Column '{col}' contains non-integer decimal values: {bad_vals}")
            df[col] = numeric_series.astype("Int64")
    return df

# === Extract Table Name from Excel/PDF File Name ===
def extract_table_name(excel_path):
    """
    Example: DraftList_Ward_28_KDMC.xlsx -> Ward_28
    """
    base = os.path.splitext(os.path.basename(excel_path))[0]
    ward_match = re.search(r"Ward[_ ]?(\d+)", base, re.IGNORECASE)
    return f"Ward_{ward_match.group(1)}" if ward_match else "Ward_Unknown"

# === Insert Excel into SQL Server ===
def insert_excel_to_sql(excel_path, db_name=DB_NAME, exclude_cols=None):
    """
    Reads an Excel file and inserts it into SQL Server.
    All text columns (Marathi included) are stored as NVARCHAR.
    Integer columns remain INT.
    Replaces the table if it already exists.
    Returns (engine, table_name) for further processing.
    """
    try:
        print(f"üìÇ Reading Excel file: {excel_path}")
        df = pd.read_excel(excel_path, dtype=str)

        if df.empty:
            print("‚ö†Ô∏è Excel file is empty, nothing to insert.")
            return None, None

        if exclude_cols:
            df = df.drop(columns=exclude_cols, errors="ignore")

        # Columns that must be integers
        int_cols = [
            "New_Voter_ID", "Voter_ID", "Section_No", "List_Number",
            "Page", "Card_Index", "Prabhag_No", "Ac_no", "Age_English"
        ]
        df = enforce_integer_columns(df, int_cols)

        # Ensure database exists
        ensure_database_exists(db_name)

        # Extract table name
        table_name = extract_table_name(excel_path)

        # Connect to database
        engine = get_engine(db_name)

        # Define SQLAlchemy dtype mapping
        sql_dtype = {}
        for col in df.columns:
            if col in int_cols:
                sql_dtype[col] = types.INTEGER()
            else:
                sql_dtype[col] = types.NVARCHAR(length=500)

        # Insert into SQL Server (replace table if exists)
        df.to_sql(
            table_name,
            engine,
            if_exists="replace",
            index=False,
            dtype=sql_dtype
        )

        print(f"‚úÖ Inserted {len(df)} rows into table '{table_name}' in database '{db_name}'")
        return engine, table_name

    except Exception as e:
        print(f"‚ùå SQL insertion failed for {excel_path}: {e}")
        return None, None

# === Add Flags ===
def add_flags(engine, table_name):
    """Add a Flag column and update values based on rules."""
    with engine.begin() as conn:
        # Add Flag column if not exists
        conn.execute(text(f"""
            IF COL_LENGTH('{table_name}', 'Flag') IS NULL
                ALTER TABLE {table_name} ADD Flag VARCHAR(255);
        """))

        # Ensure Missing_Successors column exists
        conn.execute(text(f"""
            IF COL_LENGTH('{table_name}', 'Missing_Successors') IS NULL
            BEGIN
                ALTER TABLE {table_name}
                ADD Missing_Successors INT NULL;
            END
        """))

        # Update Missing_Successors
        conn.execute(text(f"""
        ;WITH Sorted AS (
            SELECT New_Voter_ID,
                   LEAD(New_Voter_ID) OVER (ORDER BY New_Voter_ID ASC) AS next_id
            FROM {table_name}
        )
        UPDATE t
        SET Missing_Successors = CASE 
                                    WHEN s.next_id - t.New_Voter_ID > 1 
                                    THEN s.next_id - t.New_Voter_ID - 1
                                    ELSE 0
                                  END
        FROM {table_name} t
        JOIN Sorted s
          ON t.New_Voter_ID = s.New_Voter_ID;
        """))

        # Update Flag column
        conn.execute(text(f"""
        UPDATE {table_name}
        SET Flag = NULLIF(
            CONCAT_WS(',',
                CASE WHEN Voter_Name IS NULL OR Voter_Name = '' THEN 'MISSING_VN' END,
                CASE WHEN Relation_Name IS NULL OR Relation_Name = '' THEN 'MISSING_RN' END,
                CASE WHEN EPIC_Number IS NULL OR EPIC_Number = '' THEN 'MISSING_EPIC' END,
                CASE WHEN LEN(LTRIM(RTRIM(Voter_Name))) < 4 THEN 'VN_SHORT' END,
                CASE WHEN LEN(LTRIM(RTRIM(Relation_Name))) < 4 THEN 'RN_SHORT' END,
                CASE WHEN Voter_Last_Name <> Relation_Last_Name THEN 'VLN-RLN_MISMATCH' END
            ),
            ''
        );
        """))

        print(f"‚úÖ Flags updated in table '{table_name}'")


# --------------------------------------------
# ============ Main Execution ================
# --------------------------------------------
if __name__ == "__main__":
    total_start_time = time.time()
    checkpoint = load_checkpoint()
    pdf_headers_dict = {}

    pdf_files = [os.path.join(pdf_folder, f) for f in os.listdir(pdf_folder) if f.lower().endswith(".pdf")]
    print(f"üìÇ Found {len(pdf_files)} PDF files")

    # Filter PDFs to process (skip already completed ones)
    pdf_files_to_process = []
    checkpoint_changed = False
    for pdf_file in pdf_files:
        pdf_name = os.path.splitext(os.path.basename(pdf_file))[0]
        output_pdf_excel = os.path.join(os.path.dirname(output_excel), f"{pdf_name}.xlsx")

        if os.path.exists(output_pdf_excel):
            print(f"‚úîÔ∏è Skipping already processed PDF: {pdf_name}")
            if pdf_name in checkpoint:
                del checkpoint[pdf_name]
                checkpoint_changed = True
        else:
            pdf_files_to_process.append(pdf_file)

    # Update checkpoint file
    if checkpoint_changed:
        if checkpoint:
            with open(CHECKPOINT_FILE, "w", encoding="utf-8") as f:
                json.dump(checkpoint, f, indent=2)
        else:
            if os.path.exists(CHECKPOINT_FILE):
                os.remove(CHECKPOINT_FILE)

    print(f"üìÇ PDFs to process: {len(pdf_files_to_process)}")

    try:
        for pdf_file in pdf_files_to_process:
            start_time = time.time()
            pdf_name = os.path.splitext(os.path.basename(pdf_file))[0]
            print(f"\nüìÑ Processing: {pdf_name}")

            temp_excel = os.path.join(os.path.dirname(output_excel), f"{pdf_name}_emergency.xlsx")
            pdf_voter_details = []

            # ---------------- Extract PDF Header ----------------
            pdf_header_info = {}
            header_extracted = False
            with fitz.open(pdf_file) as doc:
                for page_number in range(1, 20):
                    page = doc[page_number - 1]
                    pix_low = page.get_pixmap(matrix=fitz.Matrix(3.0, 3.0))
                    img_low = Image.frombytes("RGB", [pix_low.width, pix_low.height], pix_low.samples)

                    if card_is_present(img_low):
                        print(f"‚úÖ Card found on page {page_number} of {pdf_name}. Extracting header...")
                        pix_full = page.get_pixmap(matrix=fitz.Matrix(zoom_factor, zoom_factor))
                        img_full = Image.frombytes("RGB", [pix_full.width, pix_full.height], pix_full.samples)
                        hdr = extract_header_info(img_full, top_margin=118.0, zoom_factor=zoom_factor)
                        pdf_header_info = {
                            "Municipal_Corporation": hdr.get("Municipal_Corporation", ""),
                            "Prabhag_No": hdr.get("Prabhag_No", ""),
                            "Prabhag_Name": hdr.get("Prabhag_Name", ""),
                            "File_Name": os.path.basename(pdf_file)
                        }
                        header_extracted = True
                        break

            if not header_extracted:
                print(f"‚ö†Ô∏è No cards found in {pdf_name}. Skipping header.")
            else:
                print(f"üìë Extracted PDF-level header for {pdf_name}: {pdf_header_info}")

            pdf_headers_dict[pdf_name] = pdf_header_info

            # ---------------- Process Pages ----------------
            with fitz.open(pdf_file) as doc:
                total_pages = len(doc)
                pages_to_iterate = list(range(20,21))  # all pages

                # Resume from checkpoint
                if pdf_name in checkpoint:
                    last_done = checkpoint[pdf_name]["last_page"]
                    print(f"üîÑ Resuming {pdf_name} from page {last_done + 1}")
                    old_emergency = checkpoint[pdf_name]["temp_excel"]
                    if os.path.exists(old_emergency):
                        df_existing = pd.read_excel(old_emergency, dtype=str)
                        pdf_voter_details.extend(df_existing.to_dict("records"))
                    pages_to_iterate = [p for p in pages_to_iterate if p > last_done]

                for page_num in pages_to_iterate:
                    page_voters = process_page(pdf_file, page_num, zoom_factor, pdf_header_info)
                    if page_voters:
                        pdf_voter_details.extend(page_voters)
                        save_checkpoint(pdf_name, page_num, temp_excel)

                    # Emergency save + checkpoint
                    # if pdf_voter_details:
                    #     df_tmp = pd.DataFrame(pdf_voter_details)
                    #     if column_order:
                    #         ordered_cols = [col for col in column_order if col in df_tmp.columns]
                    #         other_cols = [col for col in df_tmp.columns if col not in ordered_cols]
                    #         df_tmp = df_tmp[ordered_cols + other_cols]

                    #     for col in df_tmp.columns:
                    #         df_tmp[col] = df_tmp[col].astype(str)

                    #     df_tmp.to_excel(temp_excel, index=False, engine="openpyxl")
                    #     save_checkpoint(pdf_name, page_num, temp_excel)
                    #     print(f"üíæ Emergency save at page {page_num}: {temp_excel}")

            # ---------------- Final Save + SQL Insert ----------------
            if pdf_voter_details:
                df_pdf = pd.DataFrame(pdf_voter_details)
                if column_order:
                    ordered_cols = [col for col in column_order if col in df_pdf.columns]
                    other_cols = [col for col in df_pdf.columns if col not in ordered_cols]
                    df_pdf = df_pdf[ordered_cols + other_cols]

                for col in df_pdf.columns:
                    df_pdf[col] = df_pdf[col].astype(str)

                output_pdf_excel = os.path.join(os.path.dirname(output_excel), f"{pdf_name}.xlsx")
                df_pdf.to_excel(output_pdf_excel, index=False, engine="openpyxl")
                print(f"üìÑ Saved extracted data to: {output_pdf_excel}")


                try:
                    engine, table_name = insert_excel_to_sql(
                        output_pdf_excel,
                        exclude_cols=["Marathi_Text", "Paddle_Text", "Cleaned_Text", "Raw_Header_Text"]
                    )
    
                    if engine is not None and table_name is not None:
                        print(f"üì• Data successfully inserted into SQL Server table '{table_name}'!")

                        # ---------------- Add Flags ----------------
                        try:
                            add_flags(engine, table_name)  # Use dynamic table name
                            print(f"‚úÖ Flags added/updated successfully in SQL table '{table_name}'!")
                        except Exception as flag_e:
                            print(f"‚ùå Failed to add/update flags for '{table_name}': {flag_e}")

                except Exception as e:
                    print(f"‚ùå SQL insertion failed: {e}")

                # Insert into SQL: DB = Municipality, Table = Ward
                # try:
                #     insert_excel_to_sql(output_pdf_excel, exclude_cols=["Marathi_Text", "Paddle_Text","Cleaned_Text", "Raw_Header_Text"])
                #     print("üì• Data successfully inserted into SQL Server!")
                #     # ---------------- Add Flags ----------------
                #     try:
                #         from sqlalchemy import create_engine
                #         engine = create_engine(connection_string, fast_executemany=True)  # Make sure your connection string is correct
                #         add_flags(engine, "Ward")  # Replace "Ward" with your table name
                #         print("‚úÖ Flags added/updated successfully in SQL table!")
                #     except Exception as flag_e:
                #         print(f"‚ùå Failed to add/update flags: {flag_e}")
                # except Exception as e:
                #     print(f"‚ùå SQL insertion failed: {e}")

                # Cleanup checkpoint + emergency
                checkpoint = load_checkpoint()
                if pdf_name in checkpoint:
                    temp_file = checkpoint[pdf_name].get("temp_excel")
                    if temp_file and os.path.exists(temp_file):
                        os.remove(temp_file)
                        print(f"üóëÔ∏è Deleted emergency file for completed PDF: {temp_file}")
                    del checkpoint[pdf_name]

                    if checkpoint:
                        with open(CHECKPOINT_FILE, "w", encoding="utf-8") as f:
                            json.dump(checkpoint, f, indent=2)
                        print(f"‚úÖ Updated checkpoint after finishing {pdf_name}")
                    else:
                        if os.path.exists(CHECKPOINT_FILE):
                            os.remove(CHECKPOINT_FILE)
                        print(f"üóëÔ∏è Deleted checkpoint file as all PDFs are processed")

            else:
                print(f"‚ö†Ô∏è No data extracted from {pdf_name}. Skipping file save.")

            # Timing
            elapsed_time = time.time() - start_time
            h, rem = divmod(elapsed_time, 3600)
            m, s = divmod(rem, 60)
            print(f"‚è±Ô∏è Finished {pdf_name} in {int(h):02d}:{int(m):02d}:{int(s):02d}")

    except KeyboardInterrupt:
        print("\n‚ö†Ô∏è Process interrupted by user! Saving emergency progress...")
        save_progress(pdf_voter_details, column_order, temp_excel)
        print("üíæ Emergency file saved. You can resume later using checkpoint.")

    except Exception as e:
        print(f"\n‚ùå Unexpected error: {e}")
        save_progress(pdf_voter_details, column_order, temp_excel)
        print("üíæ Emergency file saved due to error.")

    # Total timing
    total_elapsed = time.time() - total_start_time
    th, rem = divmod(total_elapsed, 3600)
    tm, ts = divmod(rem, 60)
    print(f"\nüèÅ All files processed in {int(th):02d}:{int(tm):02d}:{int(ts):02d}")